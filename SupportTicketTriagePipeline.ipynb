{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL9s5sXRivnluXSYSlon+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshmi-29/ai-document-summarizer/blob/main/SupportTicketTriagePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NzrRu6MV_0y_",
        "outputId": "49bc96d0-eb0a-4352-9ef2-ed5cd38c4871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting scikit-llm\n",
            "  Downloading scikit_llm-1.4.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-llm) (1.6.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-llm) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.60.0 in /usr/local/lib/python3.12/dist-packages (from scikit-llm) (4.67.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.126.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.49.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (0.17.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (6.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (1.7.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (15.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.0->scikit-llm) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm) (2.5.0)\n",
            "Downloading scikit_llm-1.4.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-llm\n",
            "Successfully installed scikit-llm-1.4.2\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the main ML libraries (often pre-installed, but good practice)\n",
        "!pip install numpy pandas scikit-learn\n",
        "\n",
        "# 2. Install the LLM-specific libraries (these are less likely to be pre-installed)\n",
        "!pip install scikit-llm openai\n",
        "\n",
        "# Output should show \"Successfully installed...\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "# --- Corrected Imports for Scikit-LLM ---\n",
        "\n",
        "# Correct path for ZeroShotGPTClassifier (Classification)\n",
        "from skllm.models.gpt.classification.zero_shot import ZeroShotGPTClassifier\n",
        "\n",
        "# Correct path for GPTGeneration (Text-to-Text Modelling)\n",
        "\n",
        "# The rest of your imports should remain the same:\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "import json\n",
        "\n",
        "# --- SET YOUR API KEY ---\n",
        "# The Scikit-LLM library requires an OpenAI API key for its functionality\n",
        "# You should set this as an environment variable or directly in the code (not recommended for real projects)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\""
      ],
      "metadata": {
        "id": "MQRnxGjwAdie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SIMULATE DATA ---\n",
        "data = {\n",
        "    'Ticket_Description': [\n",
        "        \"My account is locked and I cannot access my billing info. This is urgent!\",\n",
        "        \"The website is slow, but I can still check out. Just a minor bug report.\",\n",
        "        \"Need to reset my password; tried the link but it failed. Please route to tech.\",\n",
        "        \"Question about the new enterprise pricing structure. Talk to Sales.\",\n",
        "        \"The database is completely down after the last update. High priority system crash!\",\n",
        "        \"Small visual glitch on the footer of the page. Low priority.\"\n",
        "    ],\n",
        "    'Priority': ['High', 'Low', 'Medium', 'Medium', 'High', 'Low'],\n",
        "    'Department': ['Billing', 'Technical', 'Technical', 'Sales', 'Technical', 'Technical'],\n",
        "    # Simulate a continuous variable for regression (Time_to_Resolution_Days)\n",
        "    'Resolution_Days': [1.5, 5.0, 3.2, 2.5, 0.8, 6.0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"--- Initial Data Sample ---\")\n",
        "print(df.head())\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# --- 2. TRAIN/TEST SPLIT ---\n",
        "# We split the data to ensure model evaluation is on unseen data\n",
        "X_train, X_test, y_priority_train, y_priority_test, y_dept_train, y_dept_test, y_res_train, y_res_test = train_test_split(\n",
        "    df['Ticket_Description'],\n",
        "    df['Priority'],\n",
        "    df['Department'],\n",
        "    df['Resolution_Days'],\n",
        "    test_size=0.5, # Small size for demo\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2Hz6U1TBbRB",
        "outputId": "3a885830-ec21-400d-e496-87a9bb1209e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data Sample ---\n",
            "                                  Ticket_Description Priority Department  \\\n",
            "0  My account is locked and I cannot access my bi...     High    Billing   \n",
            "1  The website is slow, but I can still check out...      Low  Technical   \n",
            "2  Need to reset my password; tried the link but ...   Medium  Technical   \n",
            "3  Question about the new enterprise pricing stru...   Medium      Sales   \n",
            "4  The database is completely down after the last...     High  Technical   \n",
            "\n",
            "   Resolution_Days  \n",
            "0              1.5  \n",
            "1              5.0  \n",
            "2              3.2  \n",
            "3              2.5  \n",
            "4              0.8  \n",
            "----------------------------------------\n",
            "Train samples: 3, Test samples: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. TF-IDF VECTORIZATION ---\n",
        "# TfidfVectorizer converts text into a matrix of token counts/weights (NumPy array under the hood)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "7gXBNxseBn0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. PRIORITY CLASSIFICATION (Scikit-learn) ---\n",
        "# Logistic Regression is a good baseline for text classification\n",
        "priority_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "priority_model.fit(X_train_vectorized, y_priority_train)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_priority_pred = priority_model.predict(X_test_vectorized)\n",
        "print(\"\\n--- PRIORITY CLASSIFICATION REPORT (Scikit-learn) ---\")\n",
        "print(classification_report(y_priority_test, y_priority_pred, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxEklSY9Bvkh",
        "outputId": "e1e33dc8-e52c-4557-a990-e1ea0c0038b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PRIORITY CLASSIFICATION REPORT (Scikit-learn) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.00      0.00      0.00       1.0\n",
            "         Low       0.00      0.00      0.00       2.0\n",
            "      Medium       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       3.0\n",
            "   macro avg       0.00      0.00      0.00       3.0\n",
            "weighted avg       0.00      0.00      0.00       3.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. RESOLUTION TIME REGRESSION (Scikit-learn) ---\n",
        "# Random Forest Regressor is robust and handles non-linear relationships\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "res_time_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
        "res_time_model.fit(X_train_vectorized, y_res_train)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_res_pred = res_time_model.predict(X_test_vectorized)\n",
        "\n",
        "# CORRECTED LINE: Use the dedicated function and remove 'squared=False'\n",
        "rmse = root_mean_squared_error(y_res_test, y_res_pred)\n",
        "\n",
        "print(f\"\\n--- RESOLUTION TIME PREDICTION (Scikit-learn) ---\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} days\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiyGUj6HB11Y",
        "outputId": "4fa34827-8e9a-4409-b327-dbd0568ca6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RESOLUTION TIME PREDICTION (Scikit-learn) ---\n",
            "Root Mean Squared Error (RMSE): 2.71 days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. LLM FILTERING AND EXTRACTION ---\n",
        "\n",
        "# 6.1. Filtering: Select tickets predicted as 'High' priority\n",
        "test_df = pd.DataFrame({\n",
        "    'Ticket_Description': X_test,\n",
        "    'Predicted_Priority': y_priority_pred,\n",
        "    'Predicted_Resolution': y_res_pred\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "# Filter for tickets the classical model flagged as high priority\n",
        "critical_tickets = test_df[test_df['Predicted_Priority'] == 'High'].copy()\n",
        "print(f\"\\n--- Critical Tickets Filtered for LLM: {len(critical_tickets)} ---\")\n",
        "\n",
        "if not critical_tickets.empty:\n",
        "    # 6.2. LLM Extraction Setup (GPTGeneration)\n",
        "    # This model is used for text-to-text generation and structured output\n",
        "    extractor = GPTGeneration(\n",
        "        model=\"gpt-3.5-turbo\",  # Use a suitable model\n",
        "        system_prompt=\"You are an AI analyst. Extract structured details from the support ticket text. The output MUST be a valid JSON object.\"\n",
        "    )\n",
        "\n",
        "    # The extraction prompt defines the desired JSON structure\n",
        "    extraction_prompt = \"\"\"\n",
        "    Analyze the following support ticket: '{text}'.\n",
        "    Extract the following details as a single JSON object:\n",
        "    {{\n",
        "        \"User_Goal\": \"What the user is trying to achieve (2-3 words)\",\n",
        "        \"Root_Issue\": \"The technical or billing issue described\",\n",
        "        \"Urgency_Reason\": \"The specific reason why the ticket is high priority\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Run LLM extraction on the critical tickets\n",
        "    extraction_results = []\n",
        "\n",
        "    # Use a loop for demonstration (for a real project, use `extractor.predict(list_of_texts)`)\n",
        "    for ticket in critical_tickets['Ticket_Description']:\n",
        "\n",
        "        # Format the prompt with the current ticket text\n",
        "        prompt = extraction_prompt.format(text=ticket)\n",
        "\n",
        "        # Get LLM prediction\n",
        "        llm_output = extractor.predict([prompt])[0]\n",
        "\n",
        "        try:\n",
        "            # Parse the JSON output from the LLM\n",
        "            parsed_json = json.loads(llm_output)\n",
        "            extraction_results.append(parsed_json)\n",
        "        except json.JSONDecodeError:\n",
        "            # Handle cases where the LLM might return malformed JSON\n",
        "            extraction_results.append({'Error': 'JSON Decode Failed', 'Raw_Output': llm_output})\n",
        "\n",
        "    # Combine results\n",
        "    llm_df = pd.DataFrame(extraction_results)\n",
        "    final_output = pd.concat([critical_tickets.reset_index(drop=True), llm_df], axis=1)\n",
        "\n",
        "    print(\"\\n--- FINAL ACTIONABLE INSIGHTS (LLM Structured Output) ---\")\n",
        "    print(final_output[['Ticket_Description', 'Predicted_Priority', 'User_Goal', 'Root_Issue', 'Urgency_Reason']].to_markdown(index=False))\n",
        "\n",
        "else:\n",
        "    print(\"No critical tickets found by the classical model for LLM analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6qabS1SDtY3",
        "outputId": "5cc3ff2b-5770-41d2-fd78-08a0f20cbcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Critical Tickets Filtered for LLM: 0 ---\n",
            "No critical tickets found by the classical model for LLM analysis.\n"
          ]
        }
      ]
    }
  ]
}